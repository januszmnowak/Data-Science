import numpy as np

"""Assume we have regression: ğ‘“(ğ±) = ğ‘â‚€ + ğ‘â‚ğ‘¥
We need to find the values of weights ğ‘â‚€, ğ‘â‚, that minimize the sum of squared residuals SSR = Î£áµ¢(ğ‘¦áµ¢ âˆ’ ğ‘“(ğ±áµ¢))Â² or the cost function ğ¶ = SSR / (2ğ‘›), which is mathematically more convenient than SSR"""

"""We need calculus to find the gradient of the cost function ğ¶ = Î£áµ¢(ğ‘¦áµ¢ âˆ’ ğ‘â‚€ âˆ’ ğ‘â‚ğ‘¥áµ¢)Â² / (2ğ‘›). Since you have two decision variables, ğ‘â‚€ and ğ‘â‚, the gradient âˆ‡ğ¶ is a vector with two components:
âˆ‚ğ¶/âˆ‚ğ‘â‚€ = (1/ğ‘›) Î£áµ¢(ğ‘â‚€ + ğ‘â‚ğ‘¥áµ¢ âˆ’ ğ‘¦áµ¢) = mean(ğ‘â‚€ + ğ‘â‚ğ‘¥áµ¢ âˆ’ ğ‘¦áµ¢) = mean(residual)
âˆ‚ğ¶/âˆ‚ğ‘â‚ = (1/ğ‘›) Î£áµ¢(ğ‘â‚€ + ğ‘â‚ğ‘¥áµ¢ âˆ’ ğ‘¦áµ¢) ğ‘¥áµ¢ = mean((ğ‘â‚€ + ğ‘â‚ğ‘¥áµ¢ âˆ’ ğ‘¦áµ¢) ğ‘¥áµ¢) = mean(residual*x)
rozwiazanie to vector=[b0,b1]"""

#input data and starting vector
x = np.array([5, 15, 25, 35, 45, 55])
y = np.array([5, 20, 14, 32, 22, 38])
vector=np.array([0.5, 0.5]) #vector startowy

#gradient descent parameters
learn_rate=0.001
n_iter=100000
tolerance=0.00001

def gradient(x,y,vector):
    residual=(vector[0]+vector[1]*x)-y
    return residual.mean(), (residual * x).mean()

for i in range(n_iter):
    diff=-learn_rate*np.array(gradient(x,y,vector)) #obliczam krok jako iloczyn learn_rate i gradientu ze znakiem minus
    if np.all(np.abs(diff)<=tolerance):
        break #jeÅ¼eli krok jest mniejszy od tolernacji to zatrzymuje algorytm
    vector+=diff
    print (i, vector)
